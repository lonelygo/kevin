# 又到财报季，AI芯片公司营收怎么样？

> 偷懒了几个月，其实，也没有。
>
> 自去年 11 月 30 日 `ChatGPT` 惊艳亮相，转眼间就一年了。这一年来，`AI` 世界的变化真的让人眼花缭乱，要保持跟上如此高的新知识、技术的演进节奏与快速变化，但愿大家的头发都还好。
>
> 过去搞算法的都觉得 `CV` **卷**，但是从 `AlexNet`到`ResNet`到`EffcientNet`差不多有 **7 年**的时间跨度。看看这一年与**大模型**相关的论文、项目，知识的更新\过时速度几乎要以周为单位了，真心学不过来啊。
>
> 所以，就~~专心摸鱼~~持续调研，认真学习领域技术和趋势发展，避免说外行话，以免别人说个新名词，一脸茫然。

前一阵在朋友圈看到某大佬调侃了一句：“都在拿nv做baseline~ 然后发现这个baseline各不相同”。

想不到啊想不到啊，关于“友商”的`baseline`各不相同这件事，在`AMD`和`NVIDIA`之间也出现了“神仙打架”。

这个故事，相信大家从很多公众号都能看到，我也不多说了。真没注意到的，事情其实就三句话说完了：

> `AMD`的发布会，放了一个`MI300X`对比`H100`的性能数据，有点优势，但不大，算是“小赢”；
>
> 过几天`NVIDIA`在自己官方博客上又放了一个文章，对比数据从“**被友商小赢**”反转为“**对友商的碾压优势**”；
>
> 然后，第二天`AMD`也丢出了一个文章（应该是出离愤怒了，从分析数据到做实验到写文章，一天搞定），指出对比不当的问题，对比数据变成了“**大赢**”，然后还补刀“在不断优化，发布会的数据是11月的，到目前已经取得了很大的进展”。

从吃瓜不嫌事大的角度来说，`NVIDIA`即使对`AMD`的数据有质疑，不服气，但是拿`FP8`的性能对对比人家`FP16`的数据，这一点确实有那么一点点“**向上管理**”的味道了（我是不太相信技术人员对这种对比不合理是没有认知的，估计是LD要结果，所以……）。

发布会的内容，铺天盖地的，就放两个互撕的原文链接，大家自己去看吧，怎么看这事情，自己判断吧。

[`Achieving Top Inference Performance with the NVIDIA H100 Tensor Core GPU and NVIDIA TensorRT-LLM`](https://developer.nvidia.com/blog/achieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm/)

[`Competitive performance claims and industry leading Inference performance on AMD Instinct MI300X`](https://community.amd.com/t5/instinct-accelerators/competitive-performance-claims-and-industry-leading-inference/ba-p/652304)

看热闹归看热闹，但是一个细节还是挺有意思。

## 打败魔法的魔法究竟是什么魔法

### `AMD`与`NVIDIA`的魔法对决

接着说`AMD`发布会上秀出的数据和与`H100`的对比数据，就捡重要的说：`Memory Bandwidth`与推理性能。

先看基本数据。

|       |   A100 SXM | H100 SXM   | H200 SXM    |  MI300X  |
|:-----:|:----------:|:----------:|:----------:|:--------:|
|  带宽  | 2.039TB/s | 3.35 TB/s  |   4.8 TB/s    |  5.3TB/s |
|  容量  |   80GB    |    80GB    |   141GB    |  192GB  |

带宽的提升比率是。

|  H100 vs A100 | H200 vs H100 |  MI300X vs H100 |
|:--------------:|:-------------:|:--------------:|
|     1.64      |       1.43    |        1.58     |

好了，文不如表，表不如图，就放两家性能对比图吧。

![H100 VS H200](https://www.nvidia.com/content/nvidiaGDC/us/en_US/data-center/h200/_jcr_content/root/responsivegrid/nv_container_156551879/nv_container_copy_511474119/nv_container_1494459889/nv_image.coreimg.svg/1699701485833/llm-inference-chart.svg)

图源：[NVIDIA H200 Tensor Core GPU](https://www.nvidia.com/en-us/data-center/h200/)

![MI300X VS H100](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0cc1a792-b27d-4695-95ba-e54a464a6625_2707x1160.png)

图源：[AMD MI300 Performance - Faster Than H100, But How Much?](https://www.semianalysis.com/p/amd-mi300-performance-faster-than)

由于对于测试的基准、方法、参数不一定一致，所以这种对比结果会有偏差的，只能简单理解一下这个性能提升数据的意义，不一定对。

我们只看`Llama2 13B`的性能对比，因为这个参数量单卡就能装下，不涉及多卡，事情就简单一点。

- `H200` 用 **1.43** 倍的带宽提升，换来了对比 `H100` 的 **1.4X** 倍的推理性能提升;

- `MI300X` 用 **1.58** 倍的带宽提升，换来了对比 `H100` 的 **1.2** 倍的推理性能提升;
- `H100` 用 **1.64** 倍的带宽提升，换来了对比 `A100` 的 **1.65** 倍的推理性能提升(BS=1);

> `H100` 对比 `A100`的数据没找到基于`Llama2 13B`的这样简单粗暴又直观的性能对比图。但是，数据还是替大家弄一个，显然不是我跑的，我这种穷人，哪里有资格拿`H100`跑数据玩？数据是在 [`NeMo Framework User Guide`](https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/llama.html#configuration-1-chatbot-conversation-use-case) 里发现的。

结论还要说么？要说么？

在单卡这种优化空间和手段有限的情况下，就算黄教主能拿到的性能提升，也是`HBM`给的，离开`HBM`那就啥也没有。

古人曰：`What Andy gives, Bill takes.`

现在是不是到了：`What Jensen wants，HBM gives.`

在AI芯片领域，`CUDA`生态这个护城河大家都听烦了，但是，这河有多宽？`AMD`的一个“不小心”给出了最好的注解：`ROCm`用`1.58`的带宽提升，**只**干出来`1.2`倍的单卡性能提升，显然是软件还没有把显存带宽全部压榨干净啊。

相同技术参数下，人家软硬件整体优化的底子还是很厚实的，轻松榨干硬件带来的性能收益，“赢”的理论基础不存在，“打平”已经是上限了。

要是上集群，人家再拿出来`NVLink`、`NVSwitch`与`IB`，**软硬网**结合，优化空间更大了，跟着`Nvidia`的路子走的“**魔法**”真能打败人家的**魔法组合**么？

**软件的路任重道远**，对于能怼出来堪称工程奇迹的`Chiplet`的`AMD`也一样。

看起来，`HBM`虽然很香，暂时还不是那个能“打败”魔法的魔法。

### `Groq`与`NVIDIA`的魔法对决

换个路子看看是不是打败魔法的魔法。

`Groq`大家应该不陌生，下半年“悄咪咪”的做了两件大事，其实也就是一件事：自己的记录自己反手破了而已。三个月性能提升三倍，这个优化收益真是相当可以的。

![100 Tokens](https://mma.prnewswire.com/media/2171682/Groq_Llama_100tps.jpg?w=600)

图源：[Groq™ First to Achieve 100 Tokens Per Second Per User on Meta AI's Llama-2 70B, Leading All Artificial Intelligence Solutions Providers in Inference Performance](https://www.prnewswire.com/news-releases/groq-first-to-achieve-100-tokens-per-second-per-user-on-meta-ais-llama-2-70b-leading-all-artificial-intelligence-solutions-providers-in-inference-performance-301895968.html?tc=eml_cleartime)

![300 Tokens](https://mma.prnewswire.com/media/2268766/LLM_performance_record.jpg?w=600)

图源：[Groq Sets New Large Language Model Performance Record of 300 Tokens per Second per User on Meta AI Foundational LLM, Llama-2 70B](https://www.prnewswire.com/news-releases/groq-sets-new-large-language-model-performance-record-of-300-tokens-per-second-per-user-on-meta-ai-foundational-llm-llama-2-70b-301980280.html)

`Groq`怎么做到的，我就不啰嗦了，大家可以看看这个文章：[GROQ SAYS IT CAN DEPLOY 1 MILLION AI INFERENCE CHIPS IN TWO YEARS](https://www.nextplatform.com/2023/11/27/groq-says-it-can-deploy-1-million-ai-inference-chips-in-two-years/)

至于这个`300 Tokens/s`性能怎么样，实在没找到`H100`的参考数据，前面提到的`A100`和`H100`推理性能对比数据可以拿来参考下。

要知道`Groq`的`GroqChip`用的是`14 nm`工艺，跑出来这个数据，他魔法是什么？

  - `230 MB`的片上`SRAM`，片外也没有用`DRAM`

  - 芯片间的**光互联**总带宽`330 GB/s`

别的也就不算魔法范畴了吧。

要问`230 MB`的片上`SRAM`怎么跑`70B`这样巨大的模型？答案就是：他们放出来的数据的测试环境中堆了**576**张卡。

对了`Groq`的Demo页可以公开访问了[chat.groq.com](https://chat.groq.com/)，大家可以去感受下`300 tokens per second per user`速度与激情的快乐与震撼。

BTW，其实 `MI300X`有`256MB`的 `L3 Cache`，说实话真不小了，应该还是没用好。

### 可能存在的魔法会是什么

一句废话：现阶段及可预见的一段时间内大模型训练、推理所需要最关键的**银弹**就是：“`Memory Bandwidth`”。

`GDDR7`容量大还便宜，但是内存带宽的上限目前看还是太低了，`GDDR7`带宽能不能再大点，只能等待唯三的供应商加加油。

`SRAM`很容量小且“贵”，`HBM3e`容量大但很贵，还不一定拿到产能，目前成熟方案似乎只有这两了。

前几年大家搞`AI`芯片，解决的核心问题是`compute bound`。

先把编程模型和软件的问题丢一边，单说堆峰值算力解决`compute bound`还是有很多解的。不管是做`ASIC`还是`DSA`还是`GPGPU`的技术路线，起码大家都在`ResNet50`上把“友商的主流产品”当作背景板在`PPT`和“手工调优”的代码中反复摩擦了。

提了`Groq`不提`Tenstorrent`是不合适的。`Tenstorrent`也是架构创新的好学生，而且他曾经用`RN50`把`Groq`和`NVIDIA`放一起同时当背景板的。在大模型时代`Tenstorrent`跑`Llama2 70B`的性能数据，还是等`Jim Keller`啥时候出来讲讲了。

不管兼容不兼容`CUDA`，走什么技术路线，似乎大家在大模型领域，依然有决心“超越友商”。

决心是决心，魔法是魔法。还是先召唤神龙吧。

对于`Memory Bandwidth Bound`这件事，现阶段`MoE`看起来是个解（但是，`batch size`大了之后，可能`MoE`可能会退化成一个“大模型”的参数量的访存）。“大模型”变“小模型”看起来也是条路子，但是，从逻辑上说`13B`干过`70B`更有可能的原因是`70B`不够强。

还有，`W8A16`、`W8A8`等等这些方案，都是可以降低对`DRAM`带宽和容量需求。但是，这些模型层面上的方法创新，是`AI`芯片行业的福利，大家一起享受，一起进步，一起躺赚。

所以，模型性能上的创新甚至结构的颠覆性改变，这些不是我们讨论的重点。

  - 存算肯定是个路子。但，要解决好存算的“算”这一部分算不了的计算，用什么技术路线做兜底计算，关键要能配得上存算部分的吞吐能力。

  - 私有协议的光互联显然是个路子。但，做到`TPU`的那个手艺，估计也不是那么容易的。

  - `3D`封装也是个路子。把价格便宜量又足的`DRAM`直接怼到计算`Die`的二楼去，`800平`的大平层瞬间变叠拼别墅，带宽狂飙了，散热么，可能要想想办法咯。

  - 反正大模型的计算强度有限，把算力的面积拿出来给`SRAM`也是个路子，跟着`Groq`走，让谁无路可走呢？

不负责的猜测，或许很快可以看到这样一个面向大模型的芯片产品：

> 1)`LPDDR5X` + `HBM3e` + `SRAM` 组成一个“三级”存储形态，通过软件的抽象，对外表现出**拥有巨大容量与高带宽，且成本更友好**的片外存储形态；且，
>
> 2)具有可配置，可路由，东南西北向`4`出的高带宽的片间光互联；且，
>
> 3)有与片间互联协议与带宽匹配的光交换机；且，
>
> 4)具有硬化的`Transformer Engine`、`Beam Search`、`MCTS`这些`ASIC`计算核心；且，
>
> 5)与已经干到`700W`以上的能耗大户相比，有“极低”功耗，且，
>
> 6)不论吞吐量还是延时，都有足够的整体优势；且，
>
> 7)有非常非常非常牛逼的**系统工程团队**与**软件工程团队**。

毕竟只有 `TCO` 的帐能算明白，才能实现真正的“商业化”。

前面这部分算是补个作业，把前几个月偷懒的事情搞完。

下面回到主题。

## 大家的营收目标达成了么？

说实话，我不知道：**既不知道营收目标，也不知道实际收入。**

今年大家都非常非常低调，毕竟“实体清单”的规则似乎很难揣测。

知道的越少越好。

事实上，今年就两个“热点”：**大模型**与**智算中心**。运营商么，大家测归测，测了一轮又一轮，测完好像就测完了。

### 大模型

先说大模型吧。好巧不巧，去年的那篇是11月21日写的，正好赶在`ChatGPT`之前，哪曾想到今年会是一个大家`All in AIGC`的局面。

说到`All in`，一家想都没想到的，基本从有消息就是负面，到基本上听不到消息的，居然突然就发布了“基于开源指令架构`RISC-V`打造的大模型系列一体机”，刷到公众号的文章的时候真的愣了一下。

因为，我要是没记错的话，他们的卡用的是`LPDDR4`，这带宽跑`7B`、`13B`，嗯，也不是不能跑，对吧。

年中的时候，有人问过这一波大模型的机会，哪家会机会大一点。

我给的答案是“两家”。

理由很简单，当时量产的，且软件也七七八八能用的，关键是有`HBM2`以上规格的`DRAM`，只有这两家啊（按照惯例，在讨论国产的话题的时候，遥遥领先都是单独存在，不能放在一起说）。

大模型，尤其是关注度最高的类`ChatGPT`模型，结构是那么的规整，算子就那几个，内存带宽优势明显，一家有互联，一家没互联，要这机会白给的机会都能错过，那也太说不过过去了吧？

事实上，这两家确实也是发了大力，不管是`WAIC`上秀出来的，还是出去讲的`PPT`内容，可以看出来，信心那是满满的。

至于，实际上的收入么。各种消息都有，我也不知道面对差距很大消息，怎么筛选。就当不知道好了。

除了这两家，其余厂商的竞争力，说实话看看`DRAM`型号，不管公开不公开“内存带宽”的具体数据，以我小学数学满分💯的功底，也能算出来，更别说，大家都有更专业同学在研究友商了。

话说回来，理论是理论，软件能不能跑出来那是另一回事。

听到某家我还比较看好的友商的`Llama2`的性能，我真的**沉默**了，太意外了，意外到不太理解这数据的真实性，不太像`CV`领域他们在圈内的软件口碑与跑出来的数据。但愿，我听到的是假消息，是在放烟雾弹吧。

当然，技术是技术，商务是商务。只要能用，一定有人能卖出去，这个毫无疑问。

### 智算中心

智算这个市场，《算力基础设施高质量发展行动计划》已经指明了方向，目前~~肯定~~应该是所有AI芯片公司的第一重点市场，没有之一。

时不时就有哪里有项目，谁谁谁可能中了，真真假假。要核实一个消息，有时候真难。

不过，真关心友商是不是搞定“大项目”了，去专门搜招标/中标的平台搜呗。只要搜的好，答案自然有，不过这些平台的会员价真不是个人可以承受的，`MTK`同学还是做进明年的市场预算吧。当然，有些地方项目，不一定能搜到结果，比如某司的南边的某个项目，我就搜不到。

大项目有大项目的好，大项目有大项目的难，说不定忙来忙去给黄老板打工了。

一个项目能做出来几个亿收入，多少人盯着，哪里那么简单就拿下对吧。当然，也有可能出现不可控的情况，做好的“局”给别人摘了桃子。但说实话，这种桃子好摘，不一定不好吃。

反正，不管知道不知道。智算中心项目今年确实为行业贡献了很大的收入，对个别家很有可能一单就是全年收入的`90%`了。

所以，目标达成多少，就看有没有拿下智算了。

大模型，确实有机会，也有落单的，但是量还是太小，对于“完成销售目标”这个宏大的主题来说，还是属于布局中的市场。

## 未来会怎样

再次说明：我是悲观的乐观主义者。

没有悲观的看清自己、认清形势、想通方法，“乐观”从哪里来？

`H100`、`H200`、`MI300X`、`Gaudi 2`一个比一个能打，弯道超车有没有不知道，差距越拉越大，应该是“行业共识”。

虽然，我们都有一颗不服输，不认命的心。

从架构创新到软硬件结合的差距，叠加“实体名单”的虎视眈眈，在”绝对实力”层面竞争可以留在规划中，但是，企业先要**生存**下去。这两年融资有多难，不用多说吧，年头见投资人见尾，就像给“行业大客户”送测一样🤐。

如何**在保证供应链安全、产能可控的前提下，保持最大程度的“竞争”优势**似乎是应该考虑的问题。

你有`HBM3e`，你能上`3nm`工艺，你很强，但是你死贵死贵死贵死贵的。

我只有`GDDR6`,我只用得起`12nm`，咬咬牙也能上`7nm`工艺,但是我能把有限的资源的理论性能都发挥出来。

而且，这样玩，我有成本优势啊。

两相比较，在不那么**追求极致**的场景下，我是不是能有一定**比较优势**，这些市场可能才是主打市场。

> 到此处有人可能会喷：你前面都要等“三级RAM抽象大DRAM了”，反手又说`GDDR6`也行，咋啥话都给你说了？回答就是：
>
> 前面是理想，是猜测，是预期，是想看看打败魔法的魔法是否存在。
> 
> 后面是现实，生活与社会的毒打早就教会了我要认清现实。
>
> ChatGPT刚火爆的时候，全世界都有“奇点到来”的错觉，疯狂上车LLMs，钱不是问题，问题是抢不到H100，等冷静了，发现用三百万的一个土豪金的铁盒子做日常使用，实在是太烧包了。那么自然会去考虑什么叫：T*D的**性价比**。

今天下午小米汽车的发布会，朋友圈看到一张图：![E-car.jpg](https://s2.loli.net/2023/12/28/xBTfbU9ZaoEVRD2.jpg)

在[电车香还是CUDA香？](https://mp.weixin.qq.com/s/PZZFOXj4E2YGdxNdrgKtXg)中，我提到“三大件的范式革命”给了传统国产油车和电车新势力“弯道”的机会。

我一直觉得走“兼容CUDA”这条路，某种意义上来说就如当年国产车的“皮尺部”一样，形似容易，神似呢？

前面提到的`MI300X`的`1.2`倍的数据，是`AMD`在`GPU`的并行计算领域没有技术积累还是`ROCm`“不兼容”`CUDA`，还是芯片的“账面”数据没有遥遥领先？

后发者，难免会陷入领先者有意无意制造的各种陷阱。

不知道大家下过大雪后跑过高速没有，最稳的开法一定是：**跟着前车留下的“车辙”走**，但是，如果遇到前车忽快忽慢，怎么办，是就这样跟着还是怎么找个机会超过前车？

显然是找个视线良好，路面情况能判断清楚的路段，走一条新车辙出来。

**既要**，**又要**，**还要**这种好事显然不现实，那么是不是聚焦到**只要**，**就能**。

大模型给了最好的“弯道”，对于推理，既然大家都要“手搓kernel”，事实上兼容不兼容`CUDA`无所谓，要性能最后还是依然要走到手搓的OP去。即使`NVIDIA`的`FasterTransformer`也不能指望`CUDA`与`nvcc`能提供免费的性能收益，该手搓还是必须手搓，不丢人。

希望，24～25年，国产AI芯片圈子，真有借着大模型机会脱颖而出的。

当然，也不希望看到还在“皇帝的新装”自嗨的戛然而止。

年头岁尾，还是不能免俗的。

*以下祝福来自`ChatGPT`的创作与`Gemini`的润色。*

在这个充满挑战与机遇的时代，愿我们像处理器中的智能核心一样，不断攀登技术高峰，追求卓越。

愿新的一年里，我们的思维像并行计算一样高效，创新如同AI加速芯片般日新月异。

愿我们在AI芯片的研发的征途中，不断实现新的突破，创造更多令人惊叹的成就。

祝大家新年快乐！

在新的一年里，工作顺利，生活幸福，身体健康，无裁员之忧（*这是我加的*）！


欢迎加我的微信“**doubtthings**”，欢迎交流与探讨。

欢迎关注我的公众号“**书不可尽信**”，原创文章第一时间推送。

<center>
    <img src="https://s2.loli.net/2022/11/27/WAC1ml5X8GTvOuH.jpg" style="width: 100px;">
</center>