# 告诉老莫，我想上`LLM`的车了

> ChatGPT风靡全球，New Bing快速跟上，Google慌了阵脚，百度文新一言要与多项业务整合，过去未在AI领域见到投入的大佬纷纷入局，各有头有脸的大厂纷纷表示自家有“类ChatGPT”技术或者即将推出相关产品，好不热闹。
>
> 一夜之间搜索成为大型语言模型（LLM）应用之争的最大战场。

> 春暖花开，万物复苏，再偷懒了就不太合适了。
>
> 年后这段时间，两个热点，不能不参与一下的。

其实内心是抗拒追热点写GPT相关的文字，一方面自己知道自己的斤两，能写出来啥货色，自己心里有数，不懂的事情还是少瞎BB比较好，那种张嘴就来，一本正经胡说八道的脸皮，我真没有😫；当然，各种`copy`堆出来一篇水文的能力也不是没有，但是，这样做事极其没品，没意思。

> 最近，相关文章多的都快看不过来了，好好的恶补了下GPT相关领域知识。
>
> 其中`OneFlow`、`机器之心`组织翻译了不少高质量的文章，感谢`袁进辉`和`赵云峰`团队的无私、高效贡献。
>
> 在`中国版OpenAI`、`中国版ChatGPT`呼之欲出的大好形势下，发现好像没人“唱反调”，而我似乎比较擅长热闹中保持冷静，那么我来说几句不同的观点吧。

## 第一个热点显然就是高启强

春节后，感觉一夜之间不管是朋友圈还是平时闲聊，都有非常多的讨论，感觉完全跟不上节奏了，没有什么是与世界脱节让人更慌了。

做为一个基本不看电视，偶尔刷美剧的业余时间除了看书就极其无聊老年人，没办法，还是发挥下`iPad`**买前生产力，买后爱奇艺**的正确使用方式，花了一周补补课。

结尾真的太仓促了，一下子从前面的细腻人物刻画，娓娓道来的讲故事，变成了32倍快进的感觉：看了又好像没看，细节丢失太多，跟不上节奏。看有一种说法，本来是规划99集的，大幅压缩了，如果是这样，我倒是真想看看99集的版本。

不得不说，这个剧核心演员都具备大师级的表演能力，相信他们演的也很爽快。对于主角**张颂文**的表演能力，怎么膜拜都不过分。看到有人转的一段他过去“未红之前”走红地毯的视频，衣着、仪态、处事，真像极了卖鱼时期的`高启强`，或者说，那一段的表演更接近本色，对于他的一夜爆红，我只想说一句：

> **芙蓉生在秋江上，不向东风怨未开。**

台上一分钟，台下十年功，含着金汤勺的那是个例，大多数人的成功都或多或少经历过蓄力、隐忍、等待，当然也许有很多人终其一生，都没有遇到他的“伯乐”，这就是人生吧。

个人觉得，`《狂飙》`能变成现象级剧集，以下两个“能力”可能是所有`成功因素`中最重要的：

- **产品力**：好剧本加上导演讲故事的能力，是其核心`产品力`；
- **软实力**：而一大批过去“默默无闻”的演员的高超演技就是其能成功的`软实力`。

> **我们需要意识到：现象级的产品，必有其独一无二的产品力和软实力。**

事后诸葛亮容易，要在事前就能做出准确判断、给出`怎样能成为`“热剧”结论或者“指引”就非常难了。

很多时候，我们可以有很多模型去量化分析`产品力`。遗憾的是，对于`软实力`或许可以构建一定的分析模型，比如很多年前，只要讲到**大数据**，一定会讲的两个例子：《纸牌屋》的导演`大卫·芬奇`和主演`凯文·史派西`的组合，以及超市中的`尿裤`和`啤酒`的组合。但是，这些“模型”是否稳定真的不好说。

如果，这个“传说中”的模型或者分析方法是如此有效，为什么没有更多的“纸牌屋”出现，为什么所有大型超市这两年都在规模化的关店？所以啊，有些“知识”真就是段子或者噱头，早期忽悠忽悠，让不明真相的群众**深以为然**罢了。

再比如，那些“流量小生”各个在微博可能都是随随便便上热搜的，从`人气`这个角度看，个顶个的顶流，但是，堆“流量小生”的“热播剧”你知道哪个？又见过哪个是全民追剧的？

琢磨下，我们做芯片是不是也是这个逻辑？

> 要想产品卖的好，硬件本身产品力是基础，软实力需要靠围绕软件栈建设的各种能力去体现。

情怀、愿景，在早期融资有价值，在卖货阶段，几乎毫无意义。

启动自嗨模式，除了加速崩溃外，看不到任何对卖货的帮助。

所以，对公司来说，多找一批**张颂文们**可能就是快速构建软实力的最优途径了。

## 第二个当然是当红炸子鸡ChatGPT

做为一个熟练掌握各种编程语言安装及`Hello World！`写法的资深摸鱼人士，善于使用各类工具高效率的去解决问题，也是基本技能之一。

### 先说下对几个相关产品的使用感受吧。

在`ChatGPT`网站还没那么拥挤的时候，早早就开始各种测了，也接到微信里，让不明真相的群友小小的体验了下，只能说：**卧槽，真强**。

`New Bing`放出来排队的消息后，第一时间去排队了，结果人品爆棚（当然也不排除账号比较古老，也给微软做过贡献），没想到2月11日就拿到了测试资格。正好是周六，于是在家玩了一天，只能惊呼“**卧槽，更强了**”。不过，最近几天由于“胡言乱语”，微软把对话轮次限制了，明显体验没有那么好了。

`Notion AI`申请晚了，等了好久才拿到内测资格，毕竟是一个专注协作与知识管理的笔记类产品，能力倾向非常明确：辅助写作。在文本生成上能明显的感受到语料对生成能力的帮助。

`Poe`因为是`Quora`出品的，擅长什么基本就不言而喻了，再加上默认提供的`Sage`和`Dragonfly`均由`OpenAI`提供支持，而`Claude`则由`Anthropic`技术提供支持。所以整体能力是没有多少意外的。

`稀宇科技`的`Glow`评价好像也挺高，很可惜，我觉得他们应该请一个好一个点的“产品经理”了，我没有熬过APP的启动**初始化引导**就放弃继续玩了，交互太复杂了，完全是站在技术角度设计交互，而不是站在用户体验角度考虑问题。

`心辰科技`的`Friday Chat`好像是国内目前唯一正式上线了的`中国版ChatGPT`了，试用的最大感觉只能说是“意思是到了”。明显能感受到的是数据集不够多，但是一些事实知识的答案都是错的，貌似不太应该。具体体验暂且不表，下面会说一下。

### 先说几个想法

**一孔之见，欢迎探讨。**

> 先抛开效果来说，`中国版的ChatGPT`肯定能做出来，而且会不止一家能做出来，这个没什么悬念，就看年内到底几家`发布`了。
>
> 从**知识的全面性角度**猜测，估计在很长时间内都无法追上`ChatGPT`或`New Bing`现有能力，这个很长时间也许是3～5年，也许更长，看机器翻译会不会有突破性进展了。
>
> 从**对话上下文的流畅度**来评估，估计再过一年到两年内能解决，但是考虑到上一条的原因，可能流畅度只能和自己的演化对比，而没法做平行对比。
>
> `中国版的ChatGPT`会有多强？特定领域知识也许可以打平，整体体验也许短期内，也许长期内都会比较糟糕，当然这可能不是一个纯技术问题占主导地位的问题。

#### 为什么能做出来

整个市场气氛都烘托到这里了，懂不懂的都要说站出来两句了，都有“相关技术储备”要入局了，这个像不像之前的`区块链`，`元宇宙`，`Web3`，`NFT`的气氛🤣？

当然，上面是调侃，大家可以忽略。

虽然`ChatGPT`、`GPT-3`都没有开源，但是相关论文有，猜测`OpenAI`使用的数据集的，训练方法的，怎么复现避坑的文章也不少。

`Meta`的`OPT-175B`是开源的，甚至`175B`的`Pretrained Model Weights`填个申请也是能获取到的，训练过程的[“血泪史”](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/final_update.md)也贴心的提供了，对于怎么训练大模型一定有借鉴意义的。

当然，还有`BLOOM `、`GLM-130B`这些开源项目作为参考。

以及还有`Colossal-AI`、`Energon-AI`、`Alpa`、`FlexGen`这些训练和推理的开源项目可以利用、参考。

另外，还有`OneFlow`这个对`PyTorch`友好的现成的专注分布式训练的团队在国内，在分布式这件事上，一定有更深的理解，也是可以借力的。

有一众开源项目的帮助，再加上各团队青年才俊的知识储备，不惜一切代价做这件事的时候，**一定能做成，这个毫无疑问**。

当然也不是说事情就这么简单，要是真这样，就不是**制高点**了，大家肯定不会争先恐后上车了。

想想就能有，只能去做梦，不能是做事。

**算力**的需求是第一个挑战。

多的不想了，谁家也没现成的`2000张A100`规模的集群准备好吧。事实上，别说完整集群，就是找2000张能腾出来长期给一个项目占用的土豪公司估计也没几家吧。我甚至怀疑没有一家能一下子说拿出来这么大算力。就能拿出来的，毕竟就算有卡，过去也几乎没有可能几千张卡怼成一个集群用的需求，所以，就算能咬咬牙停一些业务挤出来卡，IB网络的改造是跑不掉的，这需要点时间。

显然，直接买云计算平台的算力可能最简单直接，花钱能解决的事情，都不是大事。国内能不能买到不知道，实在不行还有美帝三家么。

复现`GPT-3`已经是一件非常难的事情了，怎么训练`ChatGPT`难度就更大了，**训练技巧**是需要面对的另一个挑战，这个不多说，我也不懂，有兴趣看看这篇：[为什么所有GPT-3复现都失败了？使用ChatGPT你应该知道这些](https://mp.weixin.qq.com/s/fWe9RtP8qe8uxMSukeAjKA) 。

再一个挑战，也许是最难解决的一个问题，那就是**数据集**了。

一方面，`OpenAI`没说他们训练数据集的细节，现在全行业应该都靠猜了，具体可以看这一篇：[ChatGPT数据集之谜](https://mp.weixin.qq.com/s/9vOc-OyqvzrO_w5LApurbg) 。更重要的是，可以轻松获取到的中文语料的绝对数量和相对质量，显然是没办法和英文语料对比的，这个问题就带来了我的第二个观点。

#### 为什么知识的全面性短期内很难追上

其实，提到中文语料的数量和质量，基本大家都明白这个问题的答案几乎是显而易见的。作为一个段子手，抖个机灵，相信没有人希望和`中国版ChatGPT`聊天的时候出现这样的对话吧：

> 我：请问你怎么看待中国团队很快就能复现并超过ChatGPT这件事？
> 
> 答：谢邀，人在美国，刚下飞机。老规矩，你应该先问“是不是”，再问“为什么”……
> 
> `多轮对话后`
>
> 我：道理我都懂，你能具体说一下怎么训练出来一个ChatGPT么？
>
> 答：you can you up，no can no BB……

所以，是不是要先设计几个模型，做做语料的清洗，这个可能是个问题。

另外，关于“封闭”、“孤岛”的问题，点到为止，没必要细说，大家都能明白我在说什么。在这个人人都觉得自己掌握了“相关技术”马上推出相关服务的时间点上，语料就是生产力啊。

估计只有等到这一波大家都做完了，发现自己做的真有点儿强差人意，才有可能做下来谈谈怎么做语料的知识共享吧。

当然实在不行就先靠机器翻译了，但是想想这样导致的“机翻味”肯定是没法解决的。

#### 为什么对话流畅度体验有可能很糟糕

这个事情没法展开说，依然点到为止，大家都是明白人，能懂。

![1677146673485.jpg](https://s2.loli.net/2023/02/23/eAONnUZa3pTocB5.jpg)

这个事情其实挺难破的。

从数据集下手，不该学的不让他学，这个数据清洗的工作先不说能不能做成。只要清洗的狠了，知识不连贯或者知识断层，想都不用想肯定训练出来一个智障。

就算高手在民间，能解决清洗后的知识连贯问题，那么还要考虑参考`LSTM`的 **遗忘门**，改造网络结构，以便在需要的时候，随时告诉模型“这事儿兄弟你要忘了哦，别再瞎说了。”毕竟，从头训练一次成本太高、太高、太高了。

这条路在我的认知中是不靠谱的，只能想别的办法。

办法其实也简单，一句话：**不许问，不回答**。

`不许问`做到很容易么，就是现在搜索引擎做的事情，这个真不难。

`不回答`这个就有点不好办了。各种测试`ChatGPT`的文章相信大家看了不少，正着不能问就反着问；正面不回答，就启发式提问，慢慢诱导回答。

人民群众的智慧是无穷的！

为了保证结果的安全，显然只能在输出文本再增加一层过滤之后返回给用户。那么，问题来了：

是整句完全拦截，还是拦截关键字？

不多说了，大家可以去`Friday Chat`体验下哪种支离破碎的对话体验。

## 对AI芯片公司是不是“重大利好”？

好几个朋友私下里问了我这个问题。

虽然身在业内，当然期望把每个渺茫的商机都能抓住，但是，不能忽悠人啊。基于我的认知，说点我的想法吧。

> 免责申明：以下判断，仅代表个人观点，与本人服务的企业无关。

### 训练芯片

客观说，年内国产芯片能训练出来一个“**能用的**”`ChatGPT`希望不是很大，明年或许会有一点可能，但也不是太高。理由如下：

- 软件适配，算子适配，分布式框架适配，这些都是基本功，不多说；
- 不求更大了，就`2000卡`规模的集群，国产卡除了鹏程实验室华为的Atlas 900集群，应该没有第二家有了，至少目前没听到鹏程实验室说要做这事；
- 今年大家的目标其实就一个字“**快**”发布，没有任何动力与国产卡经历漫长的适配周期，延误战机；
- 等到明年真的在大模型上尝到甜头了，后续还有足够的资金投入的个别几家，在有了“降成本”的动力的情况下，可能会考虑试试看国产卡跑跑训练；
- 当然，这有一个前提：做训练卡的起码自己内部现有一个千卡集群，能复现几个开源项目的训练，而且能明显看到训练时间和能耗的下降，才能打动用户；
- 当前融资形势如此困难，自建个千卡集群，再招一批人不计成本的去复现训练，去优化算子性能、解决集合通信的瓶颈、整体加速线性度，做出来比较好看能打动潜在客户试试看的数据，真不知道能不能玩的起了；

如果是做“自主可控”，上面的话都当我没说过，今年肯定能训出来一个模型。

### 推理芯片

这篇文章：[ChatGPT背后的经济账](https://mp.weixin.qq.com/s/aAg1ptEkQ6ahdjs-3s_g3A)大家应该都看了吧？帐还是算的很精细了，七百多个G的模型，推理成本不是一个小数字。而且考虑到，大家现在开始准备上车，到模型训练出来，正式上线，这个时间差半年起步是有的。

这个半年的时间窗口，其实就是给各家推理芯片明年收入可能的增长的最大的礼物了：

- 可以集中人力，先想办法把几个开源的模型推理跑起来，解决大模型分布式推理和大多数算子性能，确保精度不掉，性能不差；
- 但是这里其实有个问题，现在大家做的推理卡基本都是对标`T4`和`A10`的，突然要和`A100`、`A800`去PK性能了，这个TCO怎么算，要琢磨下咯；
- 好消息是`A100`有钱也买不到了，存量卡肯定优先去跑训练了，`A800`什么时候能买到也不知道，推理自然有**动力**尝试下替代方案了；
- 跑这么大的模型，在模型能用，跑起来有效果，有长期使用打算了，给模型**瘦身**，这是必然需要考虑的事情了。INT8量化不说了，标准选项，至于**剪枝**和**稀疏**能不能不掉点，估计就要靠不断尝试了；
- 在这个时间窗口中哪家能先解决量化精度不掉点，并给出一套生产级的推理部署方案，也许就真的翻身了；
- 所以我到觉得大模型这个事情，也许是**墨芯**的一个极好的机会：`INT8`+`高比例非结构化稀疏`，有机会成为推理部署降成本的杀器；

做推理芯片的兄弟们，可以上车`LLM`咯。

当然自己没搞定，或者“潜在用户”没搞定，最后可能就是消耗了大量人力，竹篮打水了。

欢迎加我的微信“**doubtthings**”，欢迎交流与探讨。

欢迎关注我的公众号“**书不可尽信**”，原创文章第一时间推送。

<center>
    <img src="https://s2.loli.net/2022/11/27/WAC1ml5X8GTvOuH.jpg" style="width: 100px;">
</center>
