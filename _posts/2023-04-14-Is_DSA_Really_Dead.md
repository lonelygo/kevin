# 无论DSA是否死了，生活还要继续

最近“知乎”上两位大神分别“探讨”了**有关DSA未来**的问题：

> **mackler**：[DSA已死](https://zhuanlan.zhihu.com/p/619717622)
>
> **夏晶晶**：[DSA死了吗？](https://zhuanlan.zhihu.com/p/620992759)

标题基本就代表了二位大佬的观点，要说核心观点的区别，不知道带一个❓算不算吧🤐。

> 另外在**semianalysis**一篇文章：[Google AI Infrastructure Supremacy: Systems Matter More Than Microarchitecture](https://www.semianalysis.com/p/google-ai-infrastructure-supremacy) 中有一段话：

> As such, hardware must be flexible to the developments of the industry and support them. The underlying hardware cannot over-specialize on any specific model architecture, or it will risk becoming obsolete as model architectures change. Chip development to large-scale volume deployment generally takes 4 years, and as such, the hardware can be left behind by what software wants to do on it. This can already be seen with certain AI accelerator architectures from startups that used a specific model type as their optimization point. This is one of the many reasons why most AI hardware startups have/will fail.

`ChatGPT`这么火，不用`GPT-4`翻译一下，都对不起一个月 20 刀的`Plus`了😭。

> 因此，硬件必须能够灵活应对行业的发展并予以支持。底层硬件不能过度专注于任何特定的模型架构，否则随着模型架构的变化，它将面临变得过时的风险。从芯片开发到大规模量产部署通常需要4年时间，因此，硬件可能会被软件在其上所需的功能所超越。我们已经可以从某些初创公司的AI加速器架构中看到这一点，这些公司将特定的模型类型作为优化重点。这是大多数AI硬件初创公司失败的众多原因之一。

这段文字在某种程度上与两位大佬文章的核心观点相互呼应，真是所见略同的即视感。将这三篇佳作放一起学习下，也是相映成趣啊。😄

BTW，单就上面这段话的翻译效果来看，`GPT-4` 好像比 `GPT-3.5-turbo` 要强一点。

再BTW，上面这篇文章，这里有翻译版本：[不仅仅是自研芯片，谷歌的AI架构解读](https://mp.weixin.qq.com/s/eivAsvSxEzRObcCWhNaA1A)，大家按需食用吧。

## GPDSA

看到这个词，先是一愣，然后就是苦笑了。必须承认**夏晶晶**发明的这个名词或许还真真真是一个本应该“存在”的名词。

走`DSA`技术路线，一方面，显然是看中了其理论上巨大`PPA`的收益，这个试图“弯道超车”的选择，对不对不好说，但是起码不能说错；另一方面，`DSA`相对`GPGPU`研发的难度~~显然~~似乎也不是一个量级，对于“求快”的创业团队，技术路线怎么选，故事怎么讲，也不难理解。

`2017`年`Google`发表了《`Attention is All You Need`》，虽然标题霸气，但在哪个时间点，估计没几个人会相信这才是打开未来AI能力的那个“银弹”（也有可能是唯一的一个）。在`2018`年那个时间点，还是**CNN**大杀四方的阶段，`RNN`领域的`LSTM`虽然在机器翻译、OCR效果不错，但是距离大规模落地，还欠点火候，`DSA`专注做好卷积、激活、池化这些计算加速似乎也没什么毛病。

当然，你可以这样怼过来：

> 一个芯片从定义到设计、量产，起码是4～5年的周期，在定义阶段**理论上**就要**预判并满足**到未来5～10年的计算特性的需求，否则很可能芯片量产的那一天，就是产品生命周期终结的那一天。

“正确的废话”怼人的时候，那是真的很好用。虽然，大家都知道这种话什么问题都解决不了，但是似乎每个公司都有一批人靠此技能生存的挺好。

但是，谁能预料到一夜之间NLP就死了，过了几天CV也要死了，这种剧烈的技术路线变化，只有神能预测了，不知道这世间有没有神。起嘛“硅仙人”看起来也没预测到这个趋势的来临。

另外，对于某些可能都没有从头到尾完成过几个模型的训练和推理应用，也没有对Nvidia的软件栈做认真使用和调研的团队来说，怎么期望有能力去预判5年以后的事情？

这一波变化来的太快、太猛、太剧烈，让我们放弃“正确的废话”，理性的来看待问题吧。

上面提到的第三篇文章中，可以深深的感受到在`Google`的**软硬件及系统与工程的协同设计下**在`TPU v4`中更激进的增加“**光互连开关**”`OCS`（optical circuit switch）、“**稀疏核**”`SC`（SparseCore）以换取`TCO`优势和在具体场景中应用中优势。

另外，上周“腾讯天籁实验室”发了一篇：[高并发低延迟降成本的战役——腾讯会议实时个性化字幕全量搭载自研紫霄芯片](https://mp.weixin.qq.com/s/fAsmnp-rkiPD9OL5x5OW4A)，这个芯片怎么回事，大家知道的都知道，也算是`DSA`在具体场景中的应用优势体现吧。

这样看，DSA死了么？好像还没死吧。

或者这么说：对于Google这种体量的互联网公司来说，只要能在核心业务（DLRM）有真金白银的收益，DSA就不会死，而且DSA的越极端，收益会更明显。

但，对于AI芯片初创公司呢？

`DSA`的`PPA`和`TCO`优势，那是必须大书特书的。也知道自己的用户知道`DSA`很显然很难做到应用领域“泛化”，也不一定会有很好的编程体验，所以“软硬件协同设计”、软件栈的“灵活、易用”都是必须要有的。

实际在做的过程，一方面，想办法把面积给`SRAM`分一点，用大缓存空间最好能把`weight`全部`cache`了，实现肉眼可见的性能巨大提升；另一方面，数据格式要做全，通用的计算单元也要想办法塞进去，否则硬件太“硬”遇到特殊计算需求就兜不住了；再一方面，编解码啊，这些友商都有的东西，不但要有，数据还不能差；最后，把这些`IP`统统塞进`SoC`，要是遇到功耗墙了，需要做出`trade-off`的时候，又不肯舍去“账面”的性能数据，在什么地方下刀子就看各家架构师的刀法和市场侧的`input`了。

只有这样做一个看似**大而全**的**GPDSA**，才能应对不同客户场景与模型需求，还要~~吹~~有很好的“编程体验”，才有可能多得到“送测”的机会。

这样别别扭扭做出来的东西，再加上天然难以搞定的软件栈和编译器，又指望和行各业的“用户”都能做进去，结果么……

这样的`DSA`死了么？好像是的。

## AIGC是不是真的是个机会

> **mackler**写了这样一句：
>
> ChatGPT横空出世之后，很多做DSA的人欢呼雀跃，觉得模型终于真正收敛到transformer is all you need了，什么可编程性都可以靠边了，软件的噩梦走了，DSA又可以牛逼起来了。

嗯，我也是欢呼雀跃了🥳。

ChatGPT出来后，很久没有认真学习的我，真认真学习了一段时间，并且得出了浅显的认识：

**模型终于有收敛迹象了，搞定几个模型走天下，大概率一定能找到饭吃的时代来了。**

欢呼雀跃过后，冷静下来。这个世界从来没有免费的饭吃，这个看起来是个机会的机会也不是白给的：

- 由于生成模型巨大的参数量，`weight`都是以`G`为单位的**巨大的模型**，这个和几年前几十`MB`撑死了一百出头的`CV`模型的性能优化，其实不是一件事情了。

- 基于模型参数量毫不讲理的极速变的巨大，推理的性能瓶颈大概率会变成`memory bandwidth bound`。所以“拍脑袋”估算性能上限，反而变的很简单了。（给非搞这一块技术的同学一句话科普下：不管你理论算力有多高、算的有多快，一个推理过程，weight 必须从 DDR 到片上缓存完成一次完整的搬运，模型大小和DDR带宽大小的简单计算结果基本就够“拍脑袋了”。算力富裕可以把 batch 搞大点，一次多算几个请求增加throughput。）

- 折腾大模型，开源的东西基本都是基于`CUDA`折腾出来的，对于`DSA`软件栈有多少能拿来就用，这是个新问题，很有可能过去的软件噩梦看起来走了一点，新的噩梦又来了。

- `BERT`、`U-Net`之前基本都优化过，`Stable Diffusion`模型也不大，两个多G，看起来应该比`GPT`那种百G模型好下手点（所以，最近好几家PR稿都提到了SD）。但是，适配是适配，生产级使用，怎么降低延时，增加吞吐，提高利用率，还有很多细致的工作要做。

- 整个优化逻辑从异构计算加速扩展到了分布式系统级别，软件有没有人能搞定，多久能搞定一个可交付的生产级的东西。

- 机会是真机会，自己的竞争优势是什么？延迟、吞吐还是认识几个人，或者就是：梦想。

我的想法：

机会是真机会，算力需求，尤其是**极具性价比的算力需求**真实存在而且有量，不管是`DSA`还是`GPGPU`大家都有机会。

模型收敛看起来是大概率事件，那么泛化能力和可编程性这些需求的优先级大概率会下移，而`TCO`会成为非常关键的指标，尤其是推理。

当然，只看硬件的`spec`可能就已经让几家有出局潜力了。

另外，最重要的是：

芯片已经回来了，也量产了，遇到这样一个计算工作负载转变的“行业盛况”，只有及时调整思路，适应市场需求这一条路。

## ~~生活~~生意还要继续

从去年开始的这个行业的投融资**寒冬**还在继续，过去一年多，不论绝对金额多寡，能拿到投资的已经很难了，尤其是再前几年把估值吹起来的，几乎都没有新的融资消息。

面向投资人的路子难以为继的时候，只有靠自己造血了，这个没什么奇怪的。

现实是`DSA`芯片已经回来了，在 `go to marking` 路上了，怎么能让他死，死无可死，能抢救就一定要抢救；不能抢救，创造条件也要抢救。

否则，真成了**最昂贵的沙子。**

`DSA`的模型泛化能力、可编程性差以及落地难的问题，这早就不是什么秘密，投资人基本不投，创业者不讲，算是共识了吧。

对于之前做`DSA`的创业公司来说，即使具有极强的人力储备和技术储备，马上就可以掉头转向`GPGPU`的`SIMT/SIMD`世界，买`IP`也好，自研也罢，反正能把`SoC`整出来。

但，客观规律不可违背，时间是无情的，未来2年怎么办？有钱再烧两年，等到新芯片回来，赌一个“王者归来”的故事？

爽剧是爽剧，生活是生活，生意是生意，醒醒。

不止`DSA`芯片，这一波AI芯片公司，只有生存下去才会有奇迹发生。今年不但要有收入，还要有说得过去、有质量的收入，恐怕才是今年会不会有奇迹（奇迹是什么，大家都懂）发生的唯一解。

如果还没有有效的市场策略，金额足够的、真实靠谱的商机储备，在这个阶段还不脚踏实地，依然给忽悠者、梦想者生存空间，幻想着突然之间就有大额订单了，沉迷于自嗨无法自拔的公司，可能真的麻烦了。

不管DSA是否会死，从最朴素的逻辑来说，：**拿钱办事，生意继续。**

只要公司还没有清盘，工资还在发，那就还是尽力帮公司把产品卖出去，争取续上下一口气。

这可能是我们从业者、打工人最大的本分了。

## DSA会死么

非自研、自用的整颗`DSA`芯片，在这一波模型收敛后，大概率没什么机会了。

但，`DSA`不会死，看看H100/800的`Tensor Core`。

欢迎加我的微信“**doubtthings**”，欢迎交流与探讨。

欢迎关注我的公众号“**书不可尽信**”，原创文章第一时间推送。

<center>
    <img src="https://s2.loli.net/2022/11/27/WAC1ml5X8GTvOuH.jpg" style="width: 100px;">
</center>